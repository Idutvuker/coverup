# Generated by ChatGPT

from pathlib import Path
import types

import pytest

from coverup import utils
import coverup.test_writer_agent as writer_agent
from coverup.test_writer_agent import build_prompt, generate_tests


def test_build_prompt_full_context_filters_and_includes(tmp_path, monkeypatch):
    # Create source and test files
    rel_src_file = Path("pkg/mod.py")
    rel_test_file = Path("tests/coverup/test_mod.py")
    src_file = tmp_path / rel_src_file
    test_file = tmp_path / rel_test_file
    src_file.parent.mkdir(parents=True, exist_ok=True)
    test_file.parent.mkdir(parents=True, exist_ok=True)
    src_file.write_text("line1\nline2\nline3", encoding="utf-8")
    test_file.write_text("old test content", encoding="utf-8")

    # Stub enumerate_lines
    def fake_enumerate_lines(text):
        return [f"{i+1}: {line}" for i, line in enumerate(text.splitlines())]

    monkeypatch.setattr(utils, "enumerate_lines", fake_enumerate_lines)

    # Patch template read for our specific template path
    original_read_text = Path.read_text

    def patched_read_text(self, *args, **kwargs):
        if str(self).endswith("coverup/prompts/create_tests.md.j2"):
            return (
                "ENUM:\n{{ enumerated_source }}\n"
                "EXIST:\n{{ existing_tests }}\n"
                "MISS:\n{{ missing_lines }}\n"
                "PFT:\n{{ previous_failed_tests }}\n"
                "PERR:\n{{ previous_errors }}\n"
                "SRC:\n{{ source_filepath }}\n"
            )
        return original_read_text(self, *args, **kwargs)

    monkeypatch.setattr(Path, "read_text", patched_read_text, raising=False)

    # File coverage stub
    class FileCoverageStub:
        def __init__(self, missing_lines):
            self.missing_lines = missing_lines

    file_coverage = FileCoverageStub([1, 3])

    # Test report stub
    class TestStub:
        def __init__(self, name, test_file):
            self.name = name
            self.test_file = test_file

    class TestReportStub:
        def __init__(self, errors):
            self._errors = errors

        def getFailedTests(self):
            return [
                TestStub(f"{str(rel_test_file)}::test_one", rel_test_file),
                TestStub("other_test.py::test_two", Path("other_test.py")),
            ]

        @property
        def errors(self):
            return self._errors

    test_report = TestReportStub(errors={rel_test_file: ["E1", "E_only_here"]})

    # Build prompt
    prompt = build_prompt(
        rel_src_file=rel_src_file,
        rel_test_file=rel_test_file,
        src_file=src_file,
        test_file=test_file,
        file_coverage=file_coverage,
        test_report=test_report,
    )

    # Assertions: includes enumerated lines, existing tests, filtered failed tests and only relevant errors
    assert "1: line1" in prompt and "3: line3" in prompt
    assert "old test content" in prompt
    assert f"{str(rel_test_file)}::test_one" in prompt and "other_test.py::test_two" not in prompt
    assert "E1" in prompt and "E_only_here" in prompt


def test_build_prompt_handles_none_and_missing_files(tmp_path, monkeypatch):
    rel_src_file = Path("pkg/absent.py")
    rel_test_file = Path("tests/coverup/test_absent.py")
    src_file = tmp_path / rel_src_file
    test_file = tmp_path / rel_test_file

    # Ensure files do not exist
    if src_file.exists():
        src_file.unlink()
    if test_file.exists():
        test_file.unlink()

    # Stub enumerate_lines to handle empty text
    def fake_enumerate_lines(text):
        return [] if not text else text.splitlines()

    monkeypatch.setattr(utils, "enumerate_lines", fake_enumerate_lines)

    # Patch template read
    original_read_text = Path.read_text

    def patched_read_text(self, *args, **kwargs):
        if str(self).endswith("coverup/prompts/create_tests.md.j2"):
            return (
                "ENUM:\n{{ enumerated_source }}\n"
                "EXIST:\n{{ existing_tests }}\n"
                "MISS:\n{{ missing_lines }}\n"
                "PFT:\n{{ previous_failed_tests }}\n"
                "PERR:\n{{ previous_errors }}\n"
                "SRC:\n{{ source_filepath }}\n"
            )
        return original_read_text(self, *args, **kwargs)

    monkeypatch.setattr(Path, "read_text", patched_read_text, raising=False)

    # Test report stub with no failures and empty errors
    class TestReportStub:
        def getFailedTests(self):
            return None

        @property
        def errors(self):
            return {}

    test_report = TestReportStub()

    prompt = build_prompt(
        rel_src_file=rel_src_file,
        rel_test_file=rel_test_file,
        src_file=src_file,
        test_file=test_file,
        file_coverage=None,
        test_report=test_report,
    )

    # Assertions for None/empty values in context
    assert "EXIST:\nNone" in prompt
    assert "MISS:\nNone" in prompt
    assert "PFT:\nNone" in prompt
    assert "PERR:\nNone" in prompt


def test_generate_tests_writes_file_and_calls_llm(tmp_path, monkeypatch):
    # Ensure tests directory exists
    (tmp_path / "tests" / "coverup").mkdir(parents=True, exist_ok=True)

    # Monkeypatch utils.create_test_file to return a specific path
    def fake_create_test_file(rel_src_file, test_dir, project_root):
        return Path("tests/coverup/test_mod_gen.py")

    monkeypatch.setattr(utils, "create_test_file", fake_create_test_file)

    # Monkeypatch build_prompt to avoid depending on filesystem/template
    monkeypatch.setattr(writer_agent, "build_prompt", lambda **kwargs: "PROMPT")

    # LLM stub
    class DummyLLM:
        model = "dummy-model"

        def __init__(self):
            self.prompts = []

        def call(self, prompt):
            self.prompts.append(prompt)
            return "LLM_TESTS"

    llm = DummyLLM()

    # Call generate_tests
    result_path = generate_tests(
        project_root=tmp_path,
        rel_src_file=Path("pkg/mod.py"),
        file_coverage=None,
        test_report=types.SimpleNamespace(getFailedTests=lambda: None, errors={}),
        llm_caller=llm,
    )

    expected_path = tmp_path / "tests/coverup/test_mod_gen.py"
    assert result_path == expected_path
    assert expected_path.read_text(encoding="utf-8") == "LLM_TESTS"
    assert llm.prompts == ["PROMPT"]